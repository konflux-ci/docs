= Creating a custom integration test

In {ProductName}, you can create your own integration tests to run on all components of a given application before they are deployed.

== Procedure

To create any custom test, complete the following steps:

. In your preferred IDE, create a Tekton pipeline in a `.yaml` file.

. Within that pipeline, create tasks, which define the actual steps of the test that {ProductName} executes against images before deploying them.

. Commit the `.yaml` file to a GitHub repo and add it as an integration test in {ProductName}.

=== Procedure with example

To create a custom test that checks that your app serves the text “Hello world!”, complete the following steps:

. In your preferred IDE, create a new `.yaml` file, with a name of your choosing.

. Define a new Tekton pipeline. The following example is the beginning of a pipeline that uses `curl` to check that the app serves the text “Hello world!”.

+
Example pipeline file:

+
[source,yaml]
----
kind: Pipeline
apiVersion: tekton.dev/v1beta1
metadata:
  name: example-pipeline
spec:
  params:
    - description: 'Snapshot of the application'
      name: SNAPSHOT
      default: '{"components": [{"name":"test-app", "containerImage": "quay.io/example/repo:latest"}]}'
      type: string
  tasks:
----

. In the `.pipeline.spec` path, declare a new task.

+
Example task declaration:

+
[source,yaml]
----
tasks:
  - name: task-1
    description: Placeholder task that prints the Snapshot and outputs standard TEST_OUTPUT
    params:
      - name: SNAPSHOT
        value: $(params.SNAPSHOT)
    taskSpec:
      params:
      - name: SNAPSHOT
      results:
      - name: TEST_OUTPUT
        description: Test output
      steps:
      - image: registry.redhat.io/openshift4/ose-cli:latest
        env:
        - name: SNAPSHOT
          value: $(params.SNAPSHOT)
        script: |
          dnf -y install jq

          echo -e "Example test task for the Snapshot:\n ${SNAPSHOT}"
          # Run custom tests for the given Snapshot here
          # After the tests finish, record the overall result in the RESULT variable
          RESULT="SUCCESS"

          # Output the standardized TEST_OUTPUT result in JSON form
          TEST_OUTPUT=$(jq -rc --arg date $(date -u --iso-8601=seconds) --arg RESULT "${RESULT}" --null-input \
            '{result: $RESULT, timestamp: $date, failures: 0, successes: 1, warnings: 0}')
          echo -n "${TEST_OUTPUT}" | tee $(results.TEST_OUTPUT.path)

----

. Save the `.yaml` file.

.. If you haven’t already, commit this file to a GitHub repository that {ProductName} can access.

+
Complete example file:

+
[source,yaml]
----
kind: Pipeline
apiVersion: tekton.dev/v1beta1
metadata:
  name: example-pipeline
spec:
  params:
    - description: 'Snapshot of the application'
      name: SNAPSHOT
      default: '{"components": [{"name":"test-app", "containerImage": "quay.io/example/repo:latest"}]}'
      type: string
  tasks:
    - name: task-1
      description: Placeholder task that prints the Snapshot and outputs standard TEST_OUTPUT
      params:
        - name: SNAPSHOT
          value: $(params.SNAPSHOT)
      taskSpec:
        params:
        - name: SNAPSHOT
        results:
        - name: TEST_OUTPUT
          description: Test output
        steps:
        - image: registry.redhat.io/openshift4/ose-cli:latest
          env:
          - name: SNAPSHOT
            value: $(params.SNAPSHOT)
          script: |
            dnf -y install jq
            echo -e "Example test task for the Snapshot:\n ${SNAPSHOT}"
            # Run custom tests for the given Snapshot here
            # After the tests finish, record the overall result in the RESULT variable
            RESULT="SUCCESS"

            # Output the standardized TEST_OUTPUT result in JSON form
            TEST_OUTPUT=$(jq -rc --arg date $(date -u --iso-8601=seconds) --arg RESULT "${RESULT}" --null-input \
              '{result: $RESULT, timestamp: $date, failures: 0, successes: 1, warnings: 0}')
            echo -n "${TEST_OUTPUT}" | tee $(results.TEST_OUTPUT.path)
----

. Add your new custom test as an integration test in {ProductName}.

.. For additional instructions on adding an integration test, see Adding an integration test.

== Customize pipelineRun definition

Integration service provides customization for both pipeline and pipelineRun definitions.
There are certain attributes that can be defined only for pipelineRuns such as:

* pipeline timeouts
* service accounts
* workspaces

If this is your case, you need to set *`Spec.ResolverRef.ResourceKind`* to 
pipelinerun(lower case `r`) within your integration test scenario definition(pipeline is being set by default). 


Example file:

[source,yaml]
---
apiVersion: appstudio.redhat.com/v1beta2
kind: IntegrationTestScenario
metadata:
  name: example-pass
  namespace: default
spec:
  application: application-sample
  contexts:
    - description: Application testing
      name: application
  resolverRef:
    resolver: git
    resourceKind: pipelinerun
    params:
      - name: url
        value: https://github.com/konflux-ci/integration-examples
      - name: revision
        value: main
      - name: pathInRepo
        value: pipelineruns/integration_pipelinerun_pass.yaml
---

== Data injected into the PipelineRun of the integration test

When you create a custom integration test, {ProductName} automatically adds certain parameters and labels to the PipelineRun of the integration test. This section explains what those parameters and labels are, and how they can help you.

Parameters:

* *`SNAPSHOT`*: contains the snapshot of the whole application as a JSON string. This JSON string provides useful information about the test, such as which components {ProductName} is testing, and what git repository and commit {ProductName} is using to build those components. For information about snapshot JSON string, see link:https://github.com/konflux-ci/integration-examples/blob/main/examples/snapshot_json_string_example[an example snapshot JSON string].

Labels:

* *`appstudio.openshift.io/application`*: contains the name of the application.

* *`appstudio.openshift.io/component`*: contains the name of the component.

* *`appstudio.openshift.io/snapshot`*: contains the name of the snapshot.

* *`test.appstudio.openshift.io/optional`*: contains the optional flag, which specifies whether or not components must pass the integration test before release.

* *`test.appstudio.openshift.io/scenario`*: contains the name of the integration test (this label ends with "scenario," because each test is technically a custom resource called an `IntegrationTestScenario`).

NOTE: It is also possible to set custom labels or annotations in the build pipelineRun, and those will be copied over
to all integration pipelineRuns associated with that build. The labels/annotations have to have the
`custom.appstudio.openshift.io` prefix in order to be copied in this manner.

=== Utilizing the labels and annotations within the integration pipeline run

It is possible to use the injected metadata within the integration PipelineRun itself in order to influence the testing behavior.
This can be done by exposing the pipelineRun labels as environment variables within a Task and then referencing them within the Task logic.

Example of extracting the component name and finding its image within the SNAPSHOT parameter's JSON data:

[source,yaml]
----
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: test-metadata
spec:
  params:
    - name: SNAPSHOT
      description: The JSON string of the Snapshot under test
  steps:
    - name: find-component-image
      image: quay.io/konflux-ci/konflux-test:stable
      workingDir: /workspace
      env:
        - name: SNAPSHOT
          value: $(params.SNAPSHOT)
        - name: COMPONENT_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['appstudio.openshift.io/component']
      script: |
        #!/bin/sh

        # Extract the component container image from the SNAPSHOT JSON data
        COMPONENT_CONTAINER_IMAGE=$(jq -r --arg component_name "${COMPONENT_NAME}" '.components[] | select(.name == $component_name) | .containerImage' <<< "${SNAPSHOT}")

        # Log the extracted variable
        echo "  COMPONENT_CONTAINER_IMAGE: ${COMPONENT_CONTAINER_IMAGE}"
----

NOTE: For more examples of available labels and how they can be used within the integration tests, consult the
link:https://github.com/konflux-ci/integration-examples/blob/main/tasks/test_metadata.yaml[example test-metadata task] as well as the
link:https://github.com/konflux-ci/integration-examples/blob/main/pipelines/integration_resolver_pipeline_pass_metadata.yaml[example integration pipeline]
which uses the information from that task's results to influence its workflow.

== Timeouts

Setting custom timeouts can be done as described in the `Timeouts` section of xref:./editing.adoc[editing a custom integration test] guide.

== Verification

After adding the integration test to an application, you need to trigger a new build of its components to make {ProductName} run the integration test. Make a commit to the GitHub repositories of your components to trigger a new build.

NOTE: For information on other ways to trigger a new build, refer to the xref:testing:integration/rerunning.adoc[Retriggering Integration Tests]

When the new build is finished, complete the following steps in the {ProductName} console:

. Go to the *Integration tests* tab and select the highlighted name of your test.

. Go to the *Pipeline runs* tab of that test and select the most recent run.

. On the *Details* page, see if the test succeeded for that component. Select the other tabs to view more details.

.. If you used our example script, switch to the *Logs* tab and verify that the test printed “Hello world!”.  

== Standardized test result

In examples above, you can see TEST_OUTPUT result being used as standardized output. This is a tekton result test outcome in json format.
TEST_OUTPUT example:
----
{"result":"SUCCESS","timestamp":"2025-04-02T01:45:00+00:00","note":"Task clair-scan completed: Refer to Tekton task result SCAN_OUTPUT for vulnerabilities scanned by Clair.","namespace":"default","successes":1,"failures":0,"warnings":0}
----

For more information about standardized tekton results in konflux, please visit
link: https://konflux-ci.dev/architecture/ADR/0030-tekton-results-naming-convention.html[Tekton Results Naming Convention]
or
xref:testing:integration/standardized-outputs.adoc[Standardized outputs]
